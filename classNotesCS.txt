dir - display current directory
cd - change directory
	e.g "cd documents" changes your directory to your documents folder 

#######################
## JUPYTER NOTEBOOKS ##
#######################

To open jupyter notebook:
	open Anaconda Prompt (Anaconda-specific command line)
	"jupyter notebook" to open up jupyter
	navigate to python class files
	(this is alternative to typing "jupyter notebook" directly into command line, which didn't work)	

	markdown cells - make the text look nice
		double click to reveal markdown code
		ctrl + enter - executes content of the cell
	press H when not in a cell to see list of shortcuts
	escape + C - makes new code cell
	escape + M - makes new markdown cell
	to insert link
		[link text in brackets](http://www.yourlinkhere.com)
	X - cut current cell
	line magics
		% lets you know you're in line magics
		%timeit can help you figure out bottlenecks in code 
		%who finds all of some kind of data (e.g. "%who list" finds all the lists you're using)
	cell magics
		%% goes at beginning of cell and applies to the entire cell
	? before function gives you documentation on it
	?? gives you even more info
	"import numpy as np"
		adds numpy library; "as np" names it np for calling later (this is standard)
		np. then calls functions from that library
			e.g. "np.random.rand" calls the random.rand function from numpy
		don't have to import entire library if only going to call one or two functions
			e.g. could do "from numpy import arange" which imports ONLY the arange function
			you can now call it directly -- don't need to use np. notation
			e.g. can write "x = arange(10)"
		but be careful with direct function imports because they could conflict with python's native functions


#####################
## INTRO TO PYTHON ##
#####################

ASSIGNING VARIABLES

	don't have to specify data type 
	variable names must start with underscore or a letter (NOT a number)
		variables case sensitive
	define a string with single OR double quotes
		x = "Obama kills a fly"
		print(x)
		prolly default to double quotes bc works with other formatting better
	declare a numerical variable
		y = 17
		print(x, y, 'times a day')
	Python is a "strongly typed" language
		Once a data type is initialized, Python will enforce that data type
		If you try to mix two different data types, it'll get mad at you
		E.g. if you're trying to add x = 2 and y = '2'
		can figure out data type with type(x)
	assigned a variable to another does NOT necessarily copy the variable
		in Python, it tends to creates a reference
			x = [1,5]
			y = x
			y[0] = 3
			now x = [3,5]
		so if you copy x into variable y, changing y ALSO changes x
		but there are ways to create unlinked copies
	can't run line by line within a cell 

CONTROL FLOW
	
	things won't work if indents aren't correct
	if, elif, & else conditions
		if x > 10: 
			print("yep, it's greater than 10")
		elif x == 10:	# elif = else if
			print("x = 10")
		else: 	# else is a catch all in case nothing was true; helps debug
			print("who knows what x is - maybe imaginary?")
	make practice to put else statement at end to catch errors

FOR LOOPS
	
	iteratively do something a fixed number of times
	for i in range(len(x)):
		print("Iteration: ", i, x[i]*y[i])

WHILE LOOPS

	keeps looping until a condition is met
		import random
		x = random.random()
		print(x)
		while x < .99:
		    x = random.random()
		    print(x)
	in this example, it keeps going as long as x is less than 0.99
	can write in a break to escape an infinite loop

LISTS, TUPLES, SETS AND DICTIONARIES

	[] for lists
	() for tuples (like lists but can't be changed)
	{} for dictionaries

	Python is 0 indexed

##################
## SECOND CLASS ##
##################

	Initializing an empty list and then filling it up with a for loop is actually super slow because you have to re-allocate memory based on the new list size every time you loop through
	Copying
		Reference -- variables are 100% tied
		View -- data shared, but operations different
		Copy -- variables are 100% independent
	Fancy indexing
		Creates an independent copy of part of an array

#################
## THIRD CLASS ##
#################

Tutorial020_BasicsFFT

	FFT
		Decomposing a signal into sinusoids and waves
		This is a linear transform of the data 
			I.e. you're not changing anything about it -- you're just transforming it from the time domain to the frequency domain
			I.e. you can move back and forth without changing the data
		Use dummy data to check your analysis pipeline
		amp = (amp / len(sw) ) * 2
			Divide by the length to normalize for the fact that the amplitude increases with the number of data points you have
			Multiply by two to get rid of negative frequencies
		Now the plot we get out confirms that our amplitude is 1 and our frequency is 10 (just as specified)
	IFFT
		The irfft function allows you to get the inverse of the fft, thus undoing the fft transform (and since it's linear, you should lose any information)
		This direction, we're going from frequency domain back to the time domain
	More complex example -- 3 sin waves
		In this example, the time domain is the top plot (hard to tell what's contributing to the signal)
		The bottom plot is in the frequency domain (you can easily see there are exactly 3 sin waves, one at 10, one at 20, and one at 30, with different amplitudes)
	Adding some "white" noise
		sw = sw + (np.random.rand(len(sw))-.5) * noise_amp
			Draws random noise from -0.5 to 0.5 that's centered on 1
		Top plot: time domain looks more noisy
		Bottom plot: frequency domain also more noisy, but clearly still pulls out three main frequencies
		If you turn up the noise, you can make the signal disappear
			E.g. noise_amp = 2 shows signal nicely
			noise_amp = 20 makes signal completely disappear
	Relationship between time & frequency domains
		Frequency > time
			The longer the time interval, the higher your frequency resolution will be
			HOWEVER, larger time bins mean you lose ability to tell WHEN something happens
		Time > frequency
			If you increase sample rate, you don't increase frequency resolution but you DO increase the range of frequencies you can estimate
		"Generally speaking, sampling your data at a higher rate is always better (because of noise)
		"
	Nyquist limit
		"You need to sample at a signal double the frequency you want to examine"
		Top plot: the "actual" signal in your brain
		Middle plot: if you sample at same frequency, you sample the peaks only (see black line), so it looks like there is NO signal at all
		Bottom plot: if you sample at double the frequency, you perfectly capture the wave (black line)
		Double the frequency you care about is the BARE MINIMUM; you can (and maybe should?) increase from there
			Noise is the reason for want even more than 2 times the frequency you care about
	Everything gets worse in the presence of noise!
		For example, take a noisy 10hz signal
			Sampled at 10hz: not remotely capturing the data
			Sampled at 20hz: just barely starting to capture (and note this is 2 times the signal)
			Sampled at 100hz: now we've perfectly captured it (this is much more than 2 times the signal, but works the best)

EXERCISES

	The point of this exercise is to show the impact of two different methods of applying FFT
		Method 1:
			FFT all trials
			Compute amplitude
			Then average across trials
		Method 2:
			Average across trials (i.e. in timme domain)
			Then FFT
			Then compute amplitude

	ROWS X COLUMNS
		Rows = trials (960 trials)
		Columns = timepoints (876 timepoints per trial)

				Timepoint 1		Timepoint 2		Timepoint 3
		Trial 1      0               0              0
		Trial 2      0               0              0 
		Trial 3      0               0              0
		Trial 4      0               0              0

		tx = time of each data acquisition

		np.mean(data, axis = 0)
			axis = 0 allows you to average across rows

##################
## COMMAND LINE ##
##################

In github GUI
	Repository --> Open in Command Prompt
git status
	tells you the status of your git
git remote -v
	origin: gives you readout of the location of your own files
	upstream: gives you origin of main think
git fetch upstream
	pulls the upstream onto your computer (into a hidden area you can't see)
git checkout -b "week_4"
	this creates a new branch for your own changes, named with whatever you put in quotes
git push -u origin "week_4"
	to commit changes
git pull upstream master
	upstream points to john's repo, master points to the main branch (required that you specify)
	green files are good -- they're ready to be committed
	red files have to be dealt with
		.ipynp_checkpoints can be removed (git rm [full file path])
git checkout --ours [file path]
	to keep YOUR version of a file

#################
## TUTORIAL030 ##
#################

	stats package in scipy
		has t-tests and other stats stuff
	alpha = 0.5
		sets the opacity in a histogram
	stats.ttest_rel(x,y,axis = 0)
		you ABSOLUTELY must specify the axis explicitly
		axis = 0 means it looks across columns
	2*(1-stats.t.cdf(x, N-1))
		computing a p-value
		could also get from stats.ttest_x
	randomization
		the null hypothesis is that condition labels don't matter
		so to test, we jumble up the condition labels
		now we can compute the probability of finding the p-value we got from our actual data by checking how likely it was to get that same p-value in our randomly generated null distribution
	rand_data = np.sign(np.random.rand(N,num_studies)-.5) * (d1-d2)
		code to randomly assign condition labels
		turns everything into 0s or 1s?
	rand_p_value = 2*(1-(np.sum(study_t_val>null_t_val) / num_randomizations))
		check the number of times our t-value exceeded the null t-values
		it's *2 because it's two-tailed 
	randomization with correlation
		now you decide whether to flip a pair of data points
		so a pair can keep its x and y coordinates or a pair can be flipped
			so that x value is now y value and vice versa
			(data points are always still paired)

#################
## TUTORIAL040 ##
#################

	When training a model, you need to either train on half the data and test on the other half, or train on one dataset and then test on other, completely separate datasets
		If you don't do this you will overfit your model (it'll be too perfect)
		Note that it's much more impressive to train on one dataset and test on other datasets (bc generalization)
	One point of this exercise is to show the importance of looking at data structure
		If you just look at your means by condition, they can be exactly the same
		But if you actually plot the data, you'll see there's actually a bunch of structure there
			In this fake dataset, that's because our two neurons respond differently to our two experimental conditions
	To deal with these data, we're going to split the data in half -- then train on one half and test on other
	How big to make your training dataset?
		If it's too small, it won't pick up on the pattern
		If it's too large, it'll overfit the model and not generalize to other datasets
	General rule is to train on 90% of data and test on other 10%
		Note that in actual practice, one would repeatedly resample a different 90% and 10% from the data (this is called cross-validation with many folds, with a fold being each time you resample a new 90% and 10%)
	Your training set NEEDS TO BE BALANCED
		E.g. same number of datapoints from each trial and condition
		Otherwise your model will be biased
	TYPES OF TRAINING
		1) Center of mass (a.k.a. euclidean distance)
			This is assignment based on MASS
			Compute the center of mass for each of our conditions
			Then for each datapoint, we'll guess which condition it's from based on which center of mass it's closer to
			This is a very simple approach
		2) Center of mass, but normalized (a.k.a. normalized euclidean distance)
			This is assignment based on MASS & VARIANCE
			Here, we weight the distance by the variance of each variable
			So if one neuron is noisier than the other (e.g., variance of 20 vs variance of 1), we want to trust the less noisy neuron more (i.e., overweight the neuron with variance of 1)
			Difference in the code here is in the distance calculation, where it's the same except that we divide by the variance (/ pooled_variance)
		3) Center of mass, but weighted by covariance structure (a.k.a. an extension of the normalized euclidean distance, called the mahalonobis distance)
			This is assignment based on MASS, VARIANCE, and COVARIANCE
			We need to account for correlational structure if we have it; otherwise we will misclassify our datapoints (because of the eliptical shape, many datapoints will be guessed as belonging to the wrong condition)
			When we normalize, we essentially "undo" the correlation
				Do this by dividing by the covariance structure
			In practice, this looks like multiplying by the inverse of the covariance matrix
				Because we can't divide by a matrix (for some linear algebra reason, idk)
				@ in the code is a stand-in for np.

#################
## TUTORIAL041 ##
#################

	DRAWING BOUNDARIES
		SVM = support vector machine
			Tries to find the line with the largest margin from each marginal datapoint
			A large criterion value ("C") will make it unlikely that marginal points make it onto the wrong side

###############
## COLORMAPS ##
###############

To improve the look of our plots, we can use colormaps!
Colormaps are from matplotlib; see link below for color libraries:	
	https://matplotlib.org/tutorials/colors/colormaps.html
I implemented a colormap in this example:
	# Get the mean across trials (axis = 0)
		meanAcrossTrials = np.mean(data, axis = 0)
	# Create color map to make plot prettier
		n = 20 # n colors
		colors = plt.cm.RdYlGn(np.linspace(0,1,n))
	# Plot mean response over time for each electrode
		for i in range(n): # get new color for each line using colormap above
    		plt.plot(tx, meanAcrossTrials.T[:,i], color=colors[i])
		plt.xlabel("Time (ms)", **figureFont)
		plt.ylabel("Mean Response", **figureFont)
		plt.show()